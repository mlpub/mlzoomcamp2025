{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c3f2e07",
   "metadata": {},
   "source": [
    "# 04 F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5385589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1269efa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# set seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# generate 1d array with value 1 and 0, size 100, with distribution 20% 1 and 80% 0\n",
    "y_target = np.random.choice([0, 1], size=100, p=[0.8, 0.2])\n",
    "print(y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c43b529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 82, 1: 18}\n"
     ]
    }
   ],
   "source": [
    "# calculate how many 1 and 0 in the array\n",
    "unique, counts = np.unique(y_target, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9121c924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 0 0 1 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1\n",
      " 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# generate simulated predictions with 20% error rate\n",
    "y_pred = y_target.copy()\n",
    "n_errors = int(0.2 * len(y_target))\n",
    "error_indices = np.random.choice(len(y_target), size=n_errors, replace=False)\n",
    "y_pred[error_indices] = 1 - y_pred[error_indices]  # flip the values\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb72bc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[65 17]\n",
      " [ 3 15]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_target, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93b08ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 15, TN: 65, FP: 17, FN: 3\n",
      "Precision: 0.47\n",
      "Recall: 0.83\n",
      "F1 Score: 0.60\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}\")\n",
    "\n",
    "# calculate precision, recall, f1 score\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b24235",
   "metadata": {},
   "source": [
    "F1 score is a balance between precision and recall.\n",
    "The ideal condition, precision and recall both are high. But in real situations, that’s rarely the case.\n",
    "\n",
    "Instead of using two separate metrics (precision and recall), we can use the F1 score as a single metric that combines both.\n",
    "It reflects how well the model maintains a balance between being accurate (precision) and comprehensive (recall).\n",
    "\n",
    "\n",
    "```\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "```\n",
    "\n",
    "The F1 score ranges from 0 to 1. \n",
    "* A value close to 1 means the model performs well, with both high precision and recall.\n",
    "* A value close to 0 means the model performs poorly on one or both metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc767f0",
   "metadata": {},
   "source": [
    "In case illegal drugs detection:\n",
    "Precision: Of the bags you stop, how many actually contain illegal stuff?\n",
    "Recall: Of all illegal bags, how many did you catch?\n",
    "\n",
    "If you stop every bag, recall is perfect, but precision is awful (you annoy everyone).\n",
    "If you stop only one bag you’re sure of → precision is perfect, but recall is awful (you missed most).\n",
    "\n",
    "A high F1 score means you stop the right bags and catch most illegal ones, a good balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a88525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c7b0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
